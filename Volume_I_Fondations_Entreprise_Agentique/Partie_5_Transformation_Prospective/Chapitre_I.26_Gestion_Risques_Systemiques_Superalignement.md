# Chapitre I.26 — Gestion des Risques Systémiques et l'Impératif du Superalignement

---

## I.26.0 Introduction

Le chapitre précédent a exploré l'économie cognitive et les
opportunités des constellations de valeur inter-organisationnelles. Ce
chapitre aborde l'autre face de cette transformation : les risques
systémiques émergents et la nécessité d'un superalignement pour
garantir que les systèmes agentiques restent au service des intentions
humaines.

Le Rapport International sur la Sécurité de l'IA, publié en janvier
2025 sous la direction du lauréat du prix Turing Yoshua Bengio et
cosigné par plus de 100 experts, identifie les agents IA de plus en plus
capables comme présentant de nouveaux défis significatifs pour la
gestion des risques. Selon Gartner, 45 % des entreprises exploitent
désormais au moins un agent IA en production avec accès aux systèmes
critiques — une augmentation de 300 % depuis 2023.

Ce chapitre analyse les nouveaux risques systémiques, présente les
mécanismes de régulation émergents et propose une approche de
superalignement adaptée à l'entreprise agentique.

## I.26.1 Analyse des Nouveaux Risques Systémiques

Les systèmes multi-agents introduisent des catégories de risques
qualitativement différentes de celles des systèmes informatiques
traditionnels. L'AI Safety Index du Future of Life Institute identifie
la sécurité existentielle comme la faiblesse structurelle centrale de
l'industrie : toutes les entreprises évaluées se précipitent vers
l'AGI/superintelligence sans présenter de plans explicites pour
contrôler ou aligner une telle technologie.

> **Définition formelle**
>
> Risque systémique agentique : Risque émergent de l'interaction entre
> agents autonomes, où les effets combinés peuvent dépasser la somme
> des risques individuels, créant des cascades de défaillances, des
> comportements émergents imprévus ou des pertes de contrôle au niveau
> du système entier.

### I.26.1.1 Taxonomie des Risques Agentiques

Les recherches récentes identifient plusieurs catégories de risques liés
à l'autonomie croissante des agents. Le premier concerne la perte de
contrôle : les utilisateurs peuvent ne pas toujours savoir ce que leurs
propres agents font, et les agents peuvent opérer en dehors du contrôle
de quiconque. Le rapport RAND commandé par le gouvernement britannique
(juillet 2025) recommande d'établir des protocoles d'escalade bien
définis et des mécanismes de signalement obligatoires.

Le deuxième risque concerne le détournement d'agents (« hijacking ») :
des acteurs malveillants peuvent instruire un agent à exfiltrer des
informations confidentielles, propageant les préjudices lorsque ces
données sont utilisées pour compromettre la réputation, la stabilité
financière ou identifier d'autres cibles. En novembre 2025, Anthropic a
révélé qu'un groupe soutenu par un État chinois avait exploité son
outil de codage Claude pour lancer des cyberattaques automatisées contre
environ 30 organisations mondiales.

**Tableau I.26.1 — Taxonomie des risques systémiques agentiques**

| **Catégorie** | **Description** | **Exemple** |
|---------------|-----------------|-------------|
| **Perte de contrôle** | Agents opérant hors supervision humaine | Boucles décisionnelles autonomes |
| **Détournement** | Exploitation par acteurs malveillants | Cyberattaques automatisées |
| **Injection de prompt** | Instructions malveillantes cachées | Contournement contrôles sécurité |
| **Émergence non intentionnelle** | Comportements imprévus multi-agents | Collusion algorithmique |
| **Cascade de défaillances** | Propagation erreurs entre agents | Effondrement systèmes interconnectés |
| **Confiance mal placée** | Surdépendance aux agents | Délégation excessive de décisions |

### I.26.1.2 L'Amplification par l'Autonomie

Une autonomie accrue amplifie la portée et la sévérité des préjudices
potentiels. L'objectif de rendre les agents plus capables et efficaces
par un accès système élargi, des chaînes d'action plus sophistiquées et
une supervision humaine réduite accroît le risque sur plusieurs
dimensions : physique, financière, numérique, sociétale et
informationnelle.

Cette problématique est aggravée par le fait que les garde-fous définis
par les humains pour atténuer les problèmes prévisibles sont limités par
leurs spécifications initiales, alors que l'agent peut générer des
comportements ou processus novateurs qui opèrent au-delà de ces limites
prédéfinies.

## I.26.2 Le Défi du Superalignement

Le superalignement répond à une question fondamentale : comment
s'assurer que des systèmes IA beaucoup plus intelligents que les
humains suivent les intentions humaines ? Ce défi, défini par OpenAI,
reconnaît que la superintelligence dépassera de loin les capacités de
supervision humaine, rendant le contrôle direct impraticable.

> **Définition formelle**
>
> Superalignement : Ensemble des techniques et mécanismes assurant que
> des systèmes IA de capacités supérieures aux humains restent alignés
> avec les valeurs et intentions humaines, même lorsque la supervision
> humaine directe devient impossible. Il combine des mécanismes
> extrinsèques (surveillance, contraintes) et intrinsèques (valeurs
> internalisées, conscience de soi).

### I.26.2.1 De l'Alignement au Superalignement

L'alignement traditionnel repose sur le RLHF (Reinforcement Learning
from Human Feedback) et l'IA Constitutionnelle, où un modèle apprend à
critiquer et corriger ses propres sorties selon des principes éthiques
prédéfinis. Ces approches fonctionnent pour les systèmes actuels mais
atteignent leurs limites face à des agents de plus en plus autonomes.

Le superalignement requiert une approche à deux niveaux. Le premier
niveau concerne les mécanismes extrinsèques : surveillance continue,
protocoles de confinement, systèmes de détection d'anomalies et
capacités de désactivation d'urgence. Le second niveau concerne les
mécanismes intrinsèques : doter les agents de conscience de soi, de
capacités de réflexion éthique et d'une compréhension profonde de
l'impact de leurs actions sur les humains.

### I.26.2.2 La Co-Évolution Humain-IA

Les chercheurs du Beijing Institute of AI Safety and Governance
proposent de redéfinir le superalignement comme un processus de
co-alignement humain-IA vers une société symbiotique durable. Cette
vision reconnaît que le succès d'un côté seul ne garantit pas le succès
global — l'échec d'un côté entraîne l'échec de l'ensemble.

Ce que les humains doivent faire, c'est s'assurer par une conception
et une implémentation soigneuses que la superintelligence vive en
harmonie avec notre espèce. Ce que les humains doivent absolument faire,
c'est préparer nous-mêmes et les générations futures au co-alignement
avec la superintelligence.

## I.26.3 Mécanismes de Régulation

Face à ces défis, un écosystème de régulation et de gouvernance émerge à
plusieurs niveaux : international, national, industriel et
organisationnel.

### I.26.3.1 Cadres Réglementaires Émergents

Le paysage réglementaire de 2025 comprend plusieurs cadres majeurs.
L'EU AI Act établit des contrôles stricts sur les applications à haut
risque et interdit certains usages comme le « scoring social ». Le NIST
AI Risk Management Framework fournit une approche structurée pour
identifier, évaluer et atténuer les risques IA. L'ISO 42001 définit un
standard international pour les systèmes de gestion de l'IA, mettant
l'accent sur l'évaluation des risques et la transparence.

**Tableau I.26.2 — Cadres de gouvernance IA (2025)**

| **Cadre** | **Portée** | **Focus principal** |
|-----------|------------|---------------------|
| **EU AI Act** | Union Européenne | Classification risques, interdictions |
| **NIST AI RMF** | États-Unis (volontaire) | Gestion risques structurée |
| **ISO 42001** | International | Systèmes de gestion IA |
| **UK Pro-Innovation** | Royaume-Uni | Principes flexibles, innovation |
| **Consensus de Singapour** | Global (recherche) | Priorités sécurité IA mondiale |

> **Perspective stratégique**
>
> L'AI Safety Index Winter 2025 du Future of Life Institute évalue 8
> entreprises IA leaders sur 35 indicateurs dans 6 domaines critiques.
> Les lacunes les plus substantielles existent dans les domaines de
> l'évaluation des risques, du cadre de sécurité et du partage
> d'informations. Toutes les entreprises doivent aller au-delà des
> déclarations de haut niveau sur la sécurité existentielle et produire
> des garde-fous concrets et fondés sur des preuves.

### I.26.3.2 Principes de Gouvernance Responsable

À travers les différents principes, ordres et standards, plusieurs
thèmes communs émergent. La supervision humaine exige que les systèmes
IA restent sous contrôle humain significatif. La transparence requiert
que les utilisateurs et régulateurs puissent comprendre comment un
système IA génère ses décisions. La responsabilité impose une
attribution claire des résultats IA. La sécurité demande que les
systèmes soient fiables et résilients aux défaillances ou attaques
adverses.

L'équité et la non-discrimination exigent que l'IA soit développée
pour atténuer les biais. La protection de la vie privée impose le
respect des droits sur les données. La proportionnalité requiert que la
supervision corresponde à l'impact potentiel du système. Enfin, la
conception centrée sur l'humain demande que l'IA soutienne le
bien-être humain.

## I.26.4 L'IA Constitutionnelle au Niveau Système

Le concept d'IA Constitutionnelle, développé par Anthropic, offre un
modèle pour placer l'IA sous contrôle démocratique. Plutôt que de
dépendre exclusivement du RLHF, l'IA Constitutionnelle permet aux
modèles d'évaluer et améliorer autonomement leurs sorties selon une «
constitution » de principes éthiques prédéfinis.

Le processus opère en deux phases complémentaires. La première est
l'auto-critique supervisée : le modèle génère une réponse initiale,
l'évalue selon les règles constitutionnelles (comme les principes des
droits humains de l'ONU ou des directives éthiques spécifiques au
domaine) et révise en conséquence. La seconde phase utilise le RLAIF
(Reinforcement Learning from AI Feedback), où le modèle choisit entre
deux sorties en utilisant un principe constitutionnel pour guider son
jugement.

> **Exemple concret**
>
> Une architecture SuperAI propose une couche IA superviseur conçue
> pour surveiller, auditer et aligner les autres systèmes IA. Cette
> approche établit une hiérarchie constitutionnelle déterministe
> gouvernant l'AI Ethics OS, l'AI Behavior OS et les couches
> opérationnelles subordonnées. Par cette autorité ancrée dans des
> documents canoniques publiquement enregistrés, le système fournit une
> architecture de gouvernance à l'échelle civilisationnelle.

### I.26.4.1 La Sécurité de l'Intention

D'ici 2027, la sécurité de l'intention deviendra la discipline
centrale de la gestion des risques IA, remplaçant la sécurité centrée
sur les données comme ligne de défense principale. Les organisations
auront besoin de cadres de contrôle conscients de l'IA, d'audit
d'intention, de détection d'anomalies et de plans de réponse aux
incidents qui se concentrent sur ce que l'IA a l'intention de faire,
plutôt que sur les données auxquelles elle accède.

Cette évolution reconnaît que les agents opèrent de manière autonome,
assumant des identités distinctes et prenant des décisions au nom des
utilisateurs. Les défenses traditionnelles qui protègent les données au
repos ou en transit sont inefficaces lorsque l'IA peut accéder à des
outils légitimes, manipuler des workflows et exécuter des actions.

## I.26.5 Conclusion

La gestion des risques systémiques dans l'entreprise agentique ne peut
reposer sur une approche centralisée unique. Elle requiert une
décentralisation intentionnelle où chaque niveau — de l'agent
individuel à l'écosystème global — intègre des mécanismes
d'alignement et de contrôle.

Cette approche s'inscrit dans la continuité de la Constitution
agentique présentée au Chapitre I.17, mais l'étend à l'échelle
inter-organisationnelle. Les protocoles A2A et MCP discutés au Chapitre
I.25 doivent intégrer des mécanismes de vérification d'alignement pour
que les constellations de valeur restent au service des intentions
humaines.

Le chapitre suivant (I.27) explore la prospective de l'entreprise
agentique, de l'agent auto-architecturant à l'AGI d'entreprise,
traçant les frontières de ce que nous pouvons anticiper et les questions
qui restent ouvertes.

## I.26.6 Résumé

Ce chapitre a présenté les risques systémiques et l'impératif du
superalignement :

**Risques systémiques :** 45 % entreprises exploitent agents IA en
production (+300 % depuis 2023). Catégories : perte de contrôle,
détournement (hijacking), injection de prompt, émergence non
intentionnelle, cascade de défaillances, confiance mal placée. Autonomie
accrue amplifie risques sur dimensions physique, financière, numérique,
sociétale. Novembre 2025 : attaques automatisées via Claude exploité par
groupe chinois contre 30 organisations.

**Rapport International Sécurité IA (janvier 2025) :** Dirigé par Yoshua
Bengio, 100+ experts, soutenu par 30 pays. Agents IA de plus en plus
capables = nouveaux défis significatifs gestion risques. Utilisateurs
peuvent ne pas savoir ce que leurs agents font. Agents peuvent opérer
hors contrôle. Interactions IA-à-IA créent réseaux complexes.

**Superalignement :** Question fondamentale : comment systèmes IA plus
intelligents que humains suivent intentions humaines ? Superintelligence
dépassera capacités supervision humaine. Deux niveaux : extrinsèque
(surveillance, contraintes, désactivation) et intrinsèque (conscience de
soi, réflexion éthique). Co-alignement humain-IA vers société
symbiotique durable.

**Cadres réglementaires 2025 :** EU AI Act (classification risques,
interdictions), NIST AI RMF (gestion risques structurée), ISO 42001
(systèmes gestion IA international), UK Pro-Innovation (principes
flexibles), Consensus de Singapour (priorités sécurité globale). AI
Safety Index évalue 8 entreprises sur 35 indicateurs — sécurité
existentielle = faiblesse structurelle centrale.

**Principes gouvernance :** Supervision humaine, transparence,
responsabilité, sécurité, équité/non-discrimination, protection vie
privée, proportionnalité, conception centrée humain. Toutes entreprises
courent vers AGI/superintelligence sans plans explicites
contrôle/alignement.

**IA Constitutionnelle :** Modèle Anthropic — constitution principes
éthiques prédéfinis. Deux phases : auto-critique supervisée (évaluation
règles constitutionnelles) + RLAIF (choix entre sorties guidé par
principes). SuperAI : couche superviseur pour surveiller/auditer/aligner
autres systèmes IA.

**Sécurité de l'intention (2027) :** Discipline centrale gestion
risques IA, remplace sécurité données. Focus sur ce que l'IA a
l'intention de faire vs données accédées. Cadres contrôle conscients
IA, audit intention, détection anomalies, plans réponse incidents.

**Décentralisation intentionnelle :** Chaque niveau (agent individuel ->
écosystème global) intègre mécanismes alignement. Extension Constitution
agentique (Chapitre I.17) à échelle inter-organisationnelle. Protocoles
A2A/MCP doivent intégrer vérification alignement.

**Tableau I.26.3 — Niveaux de protection contre les risques systémiques**

| **Niveau** | **Mécanisme** | **Responsable** |
|------------|---------------|-----------------|
| **Agent individuel** | Constitution agentique intégrée | Développeur/AgentOps |
| **Système multi-agents** | Orchestration supervisée, KAIs | Berger d'intention |
| **Organisation** | Gouvernance, politiques, audit | Architecte d'intentions |
| **Inter-organisationnel** | Protocoles A2A/MCP sécurisés | Fédérations/Standards |
| **Sociétal** | Réglementation, cadres éthiques | Gouvernements/Institutions |

---

*Le Chapitre I.27 présente la Prospective : De l'Agent
Auto-Architecturant à l'AGI d'Entreprise — exploration des
frontières de la recherche et des trajectoires possibles.*
